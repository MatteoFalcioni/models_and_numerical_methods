{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca4296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://youtu.be/TCH_1BHY58I\n",
    "# https://github.com/karpathy/makemore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e238a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now go to MLP (multilayer perceptron)....(using NLP (natural language processing))\n",
    "# 'a neural probabilistic language model' (2003) chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "# embedding of words and transfer knowledge....comment in class....\n",
    "# fig 1: 4th word predicted after the three....\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as  F\n",
    "import matplotlib.pyplot as plt #for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e6dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the words\n",
    "random.seed(158)\n",
    "words=open('Data/nomi_italiani.txt','r').read().splitlines()\n",
    "random.shuffle(words)\n",
    "words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c17ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3506c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary of characters and mapping to/from integers\n",
    "chars=sorted(list(set(''.join(words))))\n",
    "\n",
    "stoi={s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.']=0\n",
    "itos ={i:s for s,i in stoi.items()}\n",
    "print(itos)\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae3f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size =3 #context length: how many characters do we take to predict the next one ... change it !!\n",
    "# try: block_size=1 ...Markov Chain, then try = 2 and =10\n",
    "X,Y = [],[]  # input & label\n",
    "\n",
    "for w in words[0:5]:\n",
    "    print(w)\n",
    "    context=[0]*block_size\n",
    "    for ch in w +'.':\n",
    "        ix=stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context=context[1:]+[ix]  # shift: crop and append\n",
    "X=torch.tensor(X)\n",
    "Y=torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1500c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd86d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, X.dtype,Y.shape,Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9dc058",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ee26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d08971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want a model that input X and predict Y ......\n",
    "# first  the embedding in d= 2 of the 28 th character ( in the paper d=30 with a 17k vocabulary...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4cc509",
   "metadata": {},
   "outputs": [],
   "source": [
    "C=torch.randn((28,2))  #https://pytorch.org/docs/stable/generated/torch.randn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba002d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "C[5] # equivalent to one-hot encording....!! index= neuron input...first layer...comment in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69744acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c3de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the embedding.....\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data,s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(),itos[i], ha=\"center\",va=\"center\", color=\"white\")\n",
    "    plt.grid('minor')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ffec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.one_hot(torch.tensor(5),num_classes=28)  #comment this in class....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac89a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.one_hot(torch.tensor(5),num_classes=28).float()@C   #comment this in class....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i.e. the first (linear) layer (fig. in the paper)....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d5d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbddf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How we embed the 41 3-ngram's ??\n",
    "emb=C[X] # the beauty of tensor....\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc74e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moreover C is also \"differentiable\" !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f4a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afc00aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[14,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad684c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebccee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "C[X][14,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "C[X][14,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e40a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "C[15,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6efc60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aadf1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the Layer.... x.W+ b ... so the input has dimension 6=3*2 for (say) 100 neurons...\n",
    "W1=torch.randn(6,100)\n",
    "b1=torch.randn(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33097c17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 'roughly' we want to compute emb @ W1 + b1....we need to 'concatenate'....discuss in class..\n",
    "emb @ W1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c72bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/torch.html search for concatenate...\n",
    "# we want emb[:,0,:],emb[:,1,:],emb[:,2,:]  show in class.....\n",
    "\n",
    "torch.cat([emb[:,0,:],emb[:,1,:],emb[:,2,:]],1)\n",
    "#torch.cat([emb[:,0,:],emb[:,1,:],emb[:,2,:]],1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8264cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want a code for general n-grams..... \n",
    "# use 'unbind' https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind\n",
    "len(torch.unbind(emb,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584696ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unbind(emb,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc97758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and this work fore any context length.......\n",
    "\n",
    "torch.cat(torch.unbind(emb,1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9047edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a better way....\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.Tensor.view.html\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.Tensor.stride.html\n",
    "\n",
    "# use google image and discuss in class....\n",
    "\n",
    "a=torch.arange(18)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ce939",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c12bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.view(9,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4520ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.view(2,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7833f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.view(3,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a09f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very efficient in torch....discuss in class\n",
    "a.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229b1e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.view(41,6) == torch.cat(torch.unbind(emb,1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a73e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can use\n",
    "\n",
    "h=emb.view(41,6) @ W1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16bb82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558162ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f1779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more general...\n",
    "h=emb.view(-1,6) @ W1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9faf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first layer\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.tanh.html\n",
    "\n",
    "h=torch.tanh(emb.view(-1,6) @ W1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc35f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58619c",
   "metadata": {},
   "outputs": [],
   "source": [
    " #second layer...think about dimensions....\n",
    "    \n",
    "W2=torch.randn((100,28))\n",
    "b2=torch.randn(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b211524",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits= h @ W2 +b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a67d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob=counts/counts.sum(1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f35650",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now the 'true'' forth caracher....\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0fb3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9484d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob[[0,1]]\n",
    "#prob[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625389d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob[[0,1],[2,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ebd310",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c12f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob[torch.arange(41),Y]  #comments the results...random weights.....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128709ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss= - prob[torch.arange(41),Y].log().mean()\n",
    "loss  # very bad of course....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83363e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- now we made it respectable .......\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape #data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e94c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "g=torch.Generator().manual_seed(123456780)# for reproducibility\n",
    "C=torch.randn((28,2), generator=g)\n",
    "W1=torch.randn((6,100), generator=g)\n",
    "b1=torch.randn(100, generator=g)\n",
    "W2=torch.randn((100,28), generator=g)\n",
    "b2=torch.randn(28, generator=g)\n",
    "parameters=[C,W1,b1,W2,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c58c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.nelement() for p in parameters)# number of parameter in total..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bbb813",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb=C[X] # torch.Size([41, 3, 2])\n",
    "h=torch.tanh(emb.view(-1,6) @ W1 + b1) #(41,100)\n",
    "logits= h @ W2 + b2 #(41,27)\n",
    "counts=logits.exp()\n",
    "prob=counts/counts.sum(1,keepdims=True)\n",
    "loss=-prob[torch.arange(41),Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc82d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can make it more efficient using the build in cross entropy loss function...\n",
    "F.cross_entropy(logits,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07815716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so..... https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n",
    "emb=C[X] # torch.Size([41, 3, 2])\n",
    "h=torch.tanh(emb.view(-1,6) @ W1 + b1) #(41,100)\n",
    "logits= h @ W2 + b2 #(41,27)\n",
    "#counts=logits.exp()\n",
    "#prob=counts/counts.sum(1,keepdims=True)\n",
    "#loss=-prob[torch.arange(41),Y].log().mean()\n",
    "loss=F.cross_entropy(logits,Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30fb0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two very good reasons to use 'cross_entropy': more efficient (no tensor) and substract the maximum to avoid nan....discuss....\n",
    "\n",
    "logits=torch.tensor([-5,-3,0,10]) # -100\n",
    "counts=logits.exp()\n",
    "prob=counts/counts.sum()\n",
    "print(counts)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70093b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "for _ in range(1000):\n",
    "    # now we learn...forward bass \n",
    "    emb=C[X] # torch.Size([41, 3, 2])\n",
    "    h=torch.tanh(emb.view(-1,6) @ W1 + b1) #(41,100)\n",
    "    logits= h @ W2 + b2 #(41,27)\n",
    "    loss=F.cross_entropy(logits,Y)\n",
    "#    print(loss.item())\n",
    "    # backward pass \n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "    #update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1*p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c3cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from the model.....\n",
    "\n",
    "g=torch.Generator().manual_seed(12345678+10)\n",
    "\n",
    "for _ in range(20):\n",
    "    out=[]\n",
    "    context=[0]*block_size\n",
    "    while True:\n",
    "        emb=C[torch.tensor([context])]\n",
    "        h=torch.tanh(emb.view(1,-1) @ W1 + b1) \n",
    "        logits= h @ W2 + b2\n",
    "        probs=F.softmax(logits,dim=1)\n",
    "        ix=torch.multinomial(probs,num_samples=1,generator=g).item()\n",
    "        context=context[1:]+[ix]\n",
    "        out.append(ix)\n",
    "        if ix==0:\n",
    "            break\n",
    "        \n",
    "    print(''.join(itos[i] for i in out))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6074e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 'only' 41 examples and 3584 parameters we are overfitting !!\n",
    "\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "for _ in range(1000):\n",
    "    # now we learn...forward bass \n",
    "    emb=C[X] # torch.Size([41, 3, 2])\n",
    "    h=torch.tanh(emb.view(-1,6) @ W1 + b1) #(41,100)\n",
    "    logits= h @ W2 + b2 #(41,27)\n",
    "    loss=F.cross_entropy(logits,Y)\n",
    "    #print(loss.item())\n",
    "    # backward pass \n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "    #update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1*p.grad\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca4bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d356289",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y # 'almost' perfect fitting....discuss in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db780882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset...using NOW all words...\n",
    "\n",
    "block_size =3 #context length: how many characters do we take to predict the next one ... change it !!\n",
    "X,Y = [],[]  # input & label\n",
    "\n",
    "#for w in words[0:5]:    before....\n",
    "for w in words:\n",
    "  #  print(w)\n",
    "    context=[0]*block_size\n",
    "    for ch in w +'.':\n",
    "        ix=stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "      #  print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context=context[1:]+[ix]  # shift: crop and append\n",
    "X=torch.tensor(X)\n",
    "Y=torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa5705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exactly as before....\n",
    "g=torch.Generator().manual_seed(123456780)# for reproducibility\n",
    "C=torch.randn((28,2), generator=g)\n",
    "W1=torch.randn((6,100), generator=g)\n",
    "b1=torch.randn(100, generator=g)\n",
    "W2=torch.randn((100,28), generator=g)\n",
    "b2=torch.randn(28, generator=g)\n",
    "parameters=[C,W1,b1,W2,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a723dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    # now we learn...forward bass -- = 73643\n",
    "    emb=C[X] # torch.Size([--, 3, 2])\n",
    "    h=torch.tanh(emb.view(-1,6) @ W1 + b1) #(--,100)\n",
    "    logits= h @ W2 + b2 #(--,27)\n",
    "    loss=F.cross_entropy(logits,Y)\n",
    "    print(loss.item())\n",
    "    # backward pass \n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "    #update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1*p.grad\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921206f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how it is 'slowing down'...we always update on all the examples...we are going to use mini batches as usual...\n",
    "\n",
    "ix=torch.randint(0,X.shape[0],(10,)) #try ix=torch.randint(0,X.shape[0],(10,2)) and explain\n",
    "#https://pytorch.org/docs/stable/generated/torch.randint.html\n",
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13457f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix=torch.randint(0,X.shape[0],(10,1))\n",
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957e768",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    \n",
    "    # mini batch construct of size ...\n",
    "    ix=torch.randint(0,X.shape[0],(32,))\n",
    "    # now we learn...forward bass -- = 73643\n",
    "    emb=C[X[ix]] # torch.Size([--, 3, 2])\n",
    "    h=torch.tanh(emb.view(-1,6) @ W1 + b1) #(--,100)\n",
    "    logits= h @ W2 + b2 #(--,27)\n",
    "    loss=F.cross_entropy(logits,Y[ix])\n",
    "    print(loss.item())\n",
    "    # backward pass \n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "    #update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1*p.grad\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d55ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how we define the 'learning rate' ? p.data += -0.1*p.grad\n",
    "# play with learning rate from .01 to 100.... and discuss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53b007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(50):\n",
    "    \n",
    "    # mini batch construct\n",
    "    ix=torch.randint(0,X.shape[0],(100,))\n",
    "    # now we learn...forward bass -- = 73643\n",
    "    emb=C[X[ix]] # torch.Size([--, 3, 2])\n",
    "    h=torch.tanh(emb.view(-1,6) @ W1 + b1) #(--,100)\n",
    "    logits= h @ W2 + b2 #(--,27)\n",
    "    loss=F.cross_entropy(logits,Y[ix])\n",
    "    print(loss.item())\n",
    "    # backward pass \n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "    #update\n",
    "    for p in parameters:\n",
    "        p.data += -.1*p.grad\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d496d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lre=torch.linspace(-3,0,1000)\n",
    "lrs= 10**lre\n",
    "lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefe44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    \n",
    "lri=[]\n",
    "lriex=[]\n",
    "lossi=[]\n",
    "    \n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    # mini batch construct\n",
    "    ix=torch.randint(0,X.shape[0],(100,))\n",
    "    # now we learn...forward bass -- = 73643\n",
    "    emb=C[X[ix]] # torch.Size([--, 3, 2])\n",
    "    h=torch.tanh(emb.view(-1,6) @ W1 + b1) #(--,100)\n",
    "    logits= h @ W2 + b2 #(--,27)\n",
    "    loss=F.cross_entropy(logits,Y[ix])\n",
    "    print(i,loss.item())\n",
    "    # backward pass \n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "    #update\n",
    "    lr=lrs[i]\n",
    "    # lr= .01\n",
    "    for p in parameters:\n",
    "        p.data += -lr*p.grad\n",
    "#print(loss.item())\n",
    "\n",
    "#track stats\n",
    "    lri.append(lr)\n",
    "    lriex.append(lre[i])\n",
    "    lossi.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991439c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lri,lossi)  # discuss......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lriex,lossi)  # discuss......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595916e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do already much better than bigram....almost...remember overfitting.... \n",
    "\n",
    "emb=C[X] # torch.Size([41, 3, 2])\n",
    "h=torch.tanh(emb.view(-1,6) @ W1 + b1) #(41,100)\n",
    "logits= h @ W2 + b2 #(41,27)\n",
    "loss=F.cross_entropy(logits,Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d52c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training split, dev/validation split (for hypermarameters !!), test split\n",
    "# 80%, 10%, 10%\n",
    "# be careful with the test eugene.....\n",
    "\n",
    "def build_dataset(words):\n",
    "    block_size =3 #context length: how many characters do we take to predict the next one ... change it !!\n",
    "    X,Y = [],[]  # input & label\n",
    "\n",
    "   \n",
    "    for w in words:\n",
    "        context=[0]*block_size\n",
    "        for ch in w +'.':\n",
    "            ix=stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "      #  print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "            context=context[1:]+[ix]  # shift: crop and append\n",
    "    X=torch.tensor(X)\n",
    "    Y=torch.tensor(Y)\n",
    "    print(X.shape,Y.shape)\n",
    "    return X,Y\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1=int(0.8*len(words))\n",
    "n2=int(0.9*len(words))\n",
    "\n",
    "Xtr,Ytr = build_dataset(words[:n1])\n",
    "Xdev,Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f180a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we do it again with the new datasets.......\n",
    "Xtr.shape, Ytr.shape\n",
    "\n",
    "# exactly as before....\n",
    "g=torch.Generator().manual_seed(123456780)# for reproducibility\n",
    "C=torch.randn((28,2), generator=g)\n",
    "W1=torch.randn((6,100), generator=g)\n",
    "b1=torch.randn(100, generator=g)\n",
    "W2=torch.randn((100,28), generator=g)\n",
    "b2=torch.randn(28, generator=g)\n",
    "parameters=[C,W1,b1,W2,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    \n",
    "    \n",
    "lre=torch.linspace(-3,0,1000)\n",
    "lrs= 10**lre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1035ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we train only on Xtr\n",
    "\n",
    "lri=[]\n",
    "lriex=[]\n",
    "lossi=[]\n",
    "    \n",
    "\n",
    "for i in range(10000):\n",
    "    \n",
    "    # mini batch construct\n",
    "    ix=torch.randint(0,Xtr.shape[0],(40,))\n",
    "    # now we learn...forward bass -- = 73643\n",
    "    emb=C[Xtr[ix]] # torch.Size([--, 3, 2])\n",
    "    h=torch.tanh(emb.view(-1,6) @ W1 + b1) #(--,100)\n",
    "    logits= h @ W2 + b2 #(--,27)\n",
    "    loss=F.cross_entropy(logits,Ytr[ix])\n",
    "   # print(i,loss.item())\n",
    "    # backward pass \n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "    #update\n",
    "    #lr=lrs[i]\n",
    "    lr= .1\n",
    "    for p in parameters:\n",
    "        p.data += -lr*p.grad\n",
    "print(loss.item())\n",
    "\n",
    "#track stats\n",
    "#    lri.append(lr)\n",
    "#    lriex.append(lre[i])\n",
    "#    lossi.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49673949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we evaluate on Xdev\n",
    "emb=C[Xdev]\n",
    "h=torch.tanh(emb.view(-1,6) @ W1 + b1) #(--,100)\n",
    "logits= h @ W2 + b2 #(--,27)\n",
    "loss=F.cross_entropy(logits,Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f0012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we evaluate on Xtr..... we are NOT overfitting\n",
    "emb=C[Xtr]\n",
    "h=torch.tanh(emb.view(-1,6) @ W1 + b1) #(--,100)\n",
    "logits= h @ W2 + b2 #(--,27)\n",
    "loss=F.cross_entropy(logits,Ytr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6664ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we cahnge the hyper parameters....\n",
    "# before....\n",
    "#g=torch.Generator().manual_seed(123456780)# for reproducibility\n",
    "#C=torch.randn((28,2), generator=g)\n",
    "#W1=torch.randn((6,100), generator=g)\n",
    "#b1=torch.randn(100, generator=g)\n",
    "#W2=torch.randn((100,28), generator=g)\n",
    "#b2=torch.randn(28, generator=g)\n",
    "#parameters=[C,W1,b1,W2,b2]\n",
    "\n",
    "g=torch.Generator().manual_seed(123456780)# for reproducibility\n",
    "C=torch.randn((28,2), generator=g)\n",
    "W1=torch.randn((6,300), generator=g)\n",
    "b1=torch.randn(300, generator=g)\n",
    "W2=torch.randn((300,28), generator=g)\n",
    "b2=torch.randn(28, generator=g)\n",
    "parameters=[C,W1,b1,W2,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.nelement() for p in parameters)# number of parameter in total... before 3584"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee3bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri=[]\n",
    "lriex=[]\n",
    "lossi=[]\n",
    "stepi=[]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "for i in range(10000):\n",
    "    \n",
    "    # mini batch construct\n",
    "    ix=torch.randint(0,Xtr.shape[0],(40,))\n",
    "    # now we learn...forward bass -- = 73643\n",
    "    emb=C[Xtr[ix]] # torch.Size([--, 3, 2])\n",
    "    h=torch.tanh(emb.view(-1,6) @ W1 + b1) #(--,100)\n",
    "    logits= h @ W2 + b2 #(--,27)\n",
    "    loss=F.cross_entropy(logits,Ytr[ix])\n",
    "   # print(i,loss.item())\n",
    "    # backward pass \n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "    #update\n",
    "    #lr=lrs[i]\n",
    "    lr= .1\n",
    "    for p in parameters:\n",
    "        p.data += -lr*p.grad\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8792910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stepi,lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b9d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise with different hypermarameters and learning rate.....describe and discuss results...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb30813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the embedding.....\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(),itos[i], ha=\"center\",va=\"center\", color=\"white\")\n",
    "    plt.grid('minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89362b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we scale up the embedding dimension and see what happen\n",
    "\n",
    "\n",
    "g=torch.Generator().manual_seed(123456780)# for reproducibility\n",
    "C=torch.randn((28,10), generator=g)\n",
    "W1=torch.randn((30,200), generator=g)\n",
    "b1=torch.randn(200, generator=g)\n",
    "W2=torch.randn((200,28), generator=g)\n",
    "b2=torch.randn(28, generator=g)\n",
    "parameters=[C,W1,b1,W2,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a009ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.nelement() for p in parameters)# number of parameter in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebcf936",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri=[]\n",
    "lriex=[]\n",
    "lossi=[]\n",
    "stepi=[]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8174994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do lr =0.1 and then 0.01 and look at the performance....\n",
    "\n",
    "\n",
    "for i in range(200000):\n",
    "    \n",
    "    # mini batch construct\n",
    "    ix=torch.randint(0,Xtr.shape[0],(40,))\n",
    "    # now we learn...forward bass -- = 73643\n",
    "    emb=C[Xtr[ix]] # torch.Size([--, 3, 2])\n",
    "    h=torch.tanh(emb.view(-1,30) @ W1 + b1) #(--,100)\n",
    "    logits= h @ W2 + b2 #(--,27)\n",
    "    loss=F.cross_entropy(logits,Ytr[ix])\n",
    "   # print(i,loss.item())\n",
    "    # backward pass \n",
    "    for p in parameters:\n",
    "        p.grad=None\n",
    "    loss.backward()\n",
    "    #update\n",
    "    #lr=lrs[i]\n",
    "    lr= .1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr*p.grad\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.log10().item()) # note the log10 !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c615c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stepi,lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f337a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb=C[Xtr]\n",
    "h=torch.tanh(emb.view(-1,30) @ W1 + b1) #(--,100)   30 not 6 !\n",
    "logits= h @ W2 + b2 #(--,27)\n",
    "loss=F.cross_entropy(logits,Ytr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb=C[Xdev]\n",
    "h=torch.tanh(emb.view(-1,30) @ W1 + b1) #(--,100)\n",
    "logits= h @ W2 + b2 #(--,27)\n",
    "loss=F.cross_entropy(logits,Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# several (hyper)parameters to paly with: number of layer, numbers of neurons fro layers, embedding dimensions..dimension of the batches, learning rate....\n",
    "# now you can read the cited paper on language models......\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe87be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from the model.....\n",
    "\n",
    "g=torch.Generator().manual_seed(12345678+10)\n",
    "\n",
    "for _ in range(30):\n",
    "    out=[]\n",
    "    context=[0]*block_size\n",
    "    while True:\n",
    "        emb=C[torch.tensor([context])]\n",
    "        h=torch.tanh(emb.view(1,-1) @ W1 + b1) \n",
    "        logits= h @ W2 + b2\n",
    "        probs=F.softmax(logits,dim=1)\n",
    "        ix=torch.multinomial(probs,num_samples=1,generator=g).item()\n",
    "        context=context[1:]+[ix]\n",
    "        out.append(ix)\n",
    "        if ix==0:\n",
    "            break\n",
    "        \n",
    "    print(''.join(itos[i] for i in out))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b8ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
